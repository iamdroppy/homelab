

OLLAMA_CONTAINER_NAME=ollama
OLLAMA_IMAGE=ollama/ollama:0.5.12
OLLAMA_RESTART_POLICY=unless-stopped
OLLAMA_PULL=missing
OLLAMA_STORAGE=./ollama-webui

OLLAMA_MEMORY_LIMIT=10g
OLLAMA_MEMORY_RESERVATION=6g

OLLAMA_ORIGINS=*

OLLAMA_HOST=0.0.0.0
OLLAMA_PORT=11434
GPU_DRIVER=nvidia

# Extra vars
# OLLAMA_DEBUG - Show additional debug information (e.g. OLLAMA_DEBUG=1)
OLLAMA_DEBUG=1
# OLLAMA_FLASH_ATTENTION - Enabled flash attention
OLLAMA_FLASH_ATTENTION=true
# OLLAMA_KV_CACHE_TYPE - Quantization type for the K/V cache (default: f16)
# OLLAMA_GPU_OVERHEAD - Reserve a portion of VRAM per GPU (bytes)
# OLLAMA_HOST - IP Address for the ollama server (default 127.0.0.1:11434)
# OLLAMA_KEEP_ALIVE - The duration that models stay loaded in memory (default \5m\")"
OLLAMA_KEEP_ALIVE=48h30m0s
# OLLAMA_LLM_LIBRARY - Set LLM library to bypass autodetection
# OLLAMA_LOAD_TIMEOUT - How long to allow model loads to stall before giving up (default \5m\")"
OLLAMA_LOAD_TIMEOUT=10m
# OLLAMA_MAX_LOADED_MODELS - Maximum number of loaded models per GPU
OLLAMA_MAX_LOADED_MODELS=1
# OLLAMA_MAX_QUEUE - Maximum number of queued requests
# OLLAMA_MODELS - The path to the models directory
# OLLAMA_NOHISTORY - Do not preserve readline history
# OLLAMA_NOPRUNE - Do not prune model blobs on startup
# OLLAMA_NUM_PARALLEL - Maximum number of parallel requests
# OLLAMA_ORIGINS - A comma separated list of allowed origins
# OLLAMA_SCHED_SPREAD - Always schedule model across all GPUs
# OLLAMA_MULTIUSER_CACHE - Optimize prompt caching for multi-user scenarios
OLLAMA_MULTIUSER_CACHE=true
# OLLAMA_CONTEXT_LENGTH - Context length to use unless otherwise specified (default: 2048)
# OLLAMA_NEW_ENGINE - Enable the new Ollama engine
OLLAMA_NEW_ENGINE=false
OLLAMA_TTY=true
